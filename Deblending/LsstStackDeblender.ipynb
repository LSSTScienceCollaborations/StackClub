{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the LSST Stack Multiband Deblender \n",
    "<br>Authors(s): **Fred Moolekamp** ([@fred3m](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@fred3m))\n",
    "<br>Maintainer(s): **Alex Drlica-Wagner** ([@kadrlica](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@kadrlica))\n",
    "<br>Level: **Intermediate**\n",
    "<br>Last Verified to Run: **2021-09-03**\n",
    "<br>Verified Stack Release: **w_2021_33**\n",
    "\n",
    "This tutorial is designed to illustrate how to execute the multiband deblender (*scarlet*) in the LSST stack. This includes a brief introduction to LSST stack objects including:\n",
    "\n",
    "1. Geometry classes from `lsst.geom`, such as points and boxes.\n",
    "2. Higher-level astronomical primitives from `lsst.afw`, such as the `Image`, `Exposure`, and `Psf` classes.\n",
    "3. Our core algorithmic `Task` classes, including those for source detection, deblending, and measurement.\n",
    " \n",
    "This tutorial is based on Jim Bosch's globular cluster tutorial. However, in it's present state *scarlet* is unable to process crowded fields (most likely) due to poor initial conditions for the sources in these fields. So instead we use a less dense region of the image.\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "After working through this tutorial you should be able to: \n",
    "1. Configure and run the LSST multiband deblender on a test list of objects;\n",
    "2. Understand its task context in the Data Release Production (DRP) pipeline.\n",
    "\n",
    "### Logistics\n",
    "This notebook is intended to be run on a <span style=\"color:blue;font-weight:bold\">LARGE</span> instance at `lsst-lsp-stable.ncsa.illinois.edu` or `data.lsst.cloud` from a local git clone of the [StackClub](https://github.com/LSSTScienceCollaborations/StackClub) repo.\n",
    "\n",
    "\n",
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site, host, and stack version\n",
    "! echo $EXTERNAL_INSTANCE_URL\n",
    "! echo $HOSTNAME\n",
    "! eups list -s | grep lsst_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We'll start with some standard imports of both LSST and third-party packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Familiar stack packages\n",
    "from lsst.geom import Box2I, Box2D, Point2I, Point2D, Extent2I, Extent2D\n",
    "from lsst.afw.image import Exposure, Image, PARENT\n",
    "\n",
    "# These may be less familiar objects dealing with multi-band data products\n",
    "from lsst.afw.image import MultibandExposure, MultibandImage\n",
    "from lsst.afw.detection import MultibandFootprint\n",
    "from lsst.afw.image import MultibandExposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "\n",
    "This tutorial can be run with two different data sets:\n",
    "\n",
    "1. Coadded images made from Subaru Hyper Suprime-Cam (HSC) data in the COSMOS field.  We've taken a LSST reprocessing of the HSC-SSP UltraDeep COSMOS field (see [this page](https://confluence.lsstcorp.org/display/DM/S18+HSC+PDR1+reprocessing) for information on that reprocessing, and [this page](https://hsc-release.mtk.nao.ac.jp/doc/) for the data), and added simulated stars from a scaled [SDSS catalog](http://www.sdss.org/dr14/data_access/value-added-catalogs/?vac_id=photometry-of-crowded-fields-in-sdss-for-galactic-globular-and-open-clusters).  The result is a very deep image (deeper than the 10-year LSST Deep-Wide-Fast survey, though not as deep as LSST Deep Drilling fields will be) with both a large number of galaxies and region full of stars.\n",
    "2. Coadded images from the LSST DESC DC2 simulations. We've taken a fairly characeristic region of the Wide-Fast-Deep survey from LSST DESC DC2 and chosen a region around a galaxy cluster to see how the deblending performs.\n",
    "\n",
    "We'll be retrieving data using the `Butler` tool, which manages where various datasets are stored on the filesystem (and can in principle manage datasets that aren't even stored as files, though all of these are). You can find more information on the `Butler` in a set of tutorials [Basics/ButlerTutorial.ipynb](../Basics/ButlerTutorial.ipynb) and [Basics/Gen3ButlerTutorial.ipynb](../Basics/Gen3ButlerTutorial.ipynb).\n",
    "\n",
    "We start by creating a `Butler` instance, pointing it at a *Data Repository* (which here is just a root directory). Datasets managed by a butler are identified by a dictionary *Data ID* (specifying things like the visit number or sky patch) and a string *DatasetType* (such as a particular image or catalog).  Different DatasetTypes have different keys, while different instances of the same Dataset Type have different values.  All of the datasets we use in this tutorial will correspond to the same patch of sky, so they'll have at least the keys in the dictionary in the next cell (they will also have `filter`, but with different values).\n",
    "\n",
    "We can now use those to load a set of *grizy* coadds, which we'll put directly in a dictionary.  The result of each `Butler.get` call is in this case an `lsst.afw.image.Exposure` object, an image that actually contains three \"planes\" (the main image, a bit mask, and a variance image) as well as many other objects that describe the image, such as its PSF and WCS.  Note that we (confusingly) use `Exposures` to hold coadd images as well as true single-exposure images, but combine them into a `MultibandExposure`, which contains an exposure in each band.\n",
    "\n",
    "The DatasetType here (`deepCoadd_calexp` or `deepCoadd`) is a coadd image on which we've already done some additional processing, such as subtracting the background and setting some mask values, and the extra `filter` argument gets appended to the Data ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the DC2 Gen3 repository on this site\n",
    "URL = os.getenv('EXTERNAL_INSTANCE_URL')\n",
    "if URL.endswith('data.lsst.cloud'): # IDF\n",
    "    repo = \"s3://butler-us-central1-dp01\"\n",
    "elif URL.endswith('ncsa.illinois.edu'): # NCSA\n",
    "    repo = \"/repo/dc2\"\n",
    "else:\n",
    "    raise Exception(f\"Unrecognized URL: {URL}\")\n",
    "\n",
    "collection='2.2i/runs/DP0.1'\n",
    "\n",
    "# Data set specification\n",
    "dataset_type = \"deepCoadd\"\n",
    "dataId = {\"tract\": 3828, \"patch\": 7*(4)+4}\n",
    "\n",
    "# Filters\n",
    "filters = 'grizy'\n",
    "filter_keys = filters\n",
    "# Specify the location of a cutout box\n",
    "x0,y0 = 18380, 17750\n",
    "# Blended source index for deeper investigation\n",
    "idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.daf.butler import Butler \n",
    "butler = Butler(repo,collections=collection)\n",
    "\n",
    "# Retrieve the data using the `butler` looping over the filters\n",
    "coadds = [butler.get(dataset_type, dataId=dataId, band=f) for f in filter_keys]\n",
    "coadds = MultibandExposure.fromExposures(filters, coadds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making and displaying color composite images\n",
    "\n",
    "We'll start by just looking at the images, as 3-color composites.  We'll use astropy to build those as a nice way to demonstrate how to get NumPy arrays from the `MultibandImage` objects (the images in `coadds`).  (LSST also has code to make 3-color composites using the same algorithm, and in fact the Astropy implementation is based on ours.) We'll use matplotlib to display the images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import make_lupton_rgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function a few times to display color images.  It's worth reading through the implementation carefully to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showRGB(image, bgr=\"gri\", ax=None, fp=None, figsize=(8,8), stretch=1, Q=10):\n",
    "    \"\"\"Display an RGB color composite image with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : `MultibandImage`\n",
    "        `MultibandImage` to display.\n",
    "    bgr : sequence\n",
    "        A 3-element sequence of filter names (i.e. keys of the exps dict) indicating what band\n",
    "        to use for each channel. If `image` only has three filters then this parameter is ignored\n",
    "        and the filters in the image are used.\n",
    "    ax : `matplotlib.axes.Axes`\n",
    "        Axis in a `matplotlib.Figure` to display the image.\n",
    "        If `axis` is `None` then a new figure is created.\n",
    "    fp: `lsst.afw.detection.Footprint`\n",
    "        Footprint that contains the peak catalog for peaks in the image.\n",
    "        If `fp` is `None` then no peak positions are plotted.\n",
    "    figsize: tuple\n",
    "        Size of the `matplotlib.Figure` created.\n",
    "        If `ax` is not `None` then this parameter is ignored.\n",
    "    stretch: int\n",
    "        The linear stretch of the image.\n",
    "    Q: int\n",
    "        The Asinh softening parameter.\n",
    "    \"\"\"\n",
    "    # If the image only has 3 bands, reverse the order of the bands to produce the RGB image\n",
    "    if len(image) == 3:\n",
    "        bgr = image.filters\n",
    "    # Extract the primary image component of each Exposure with the .image property, and use .array to get a NumPy array view.\n",
    "    rgb = make_lupton_rgb(image_r=image[bgr[2]].array,  # numpy array for the r channel\n",
    "                          image_g=image[bgr[1]].array,  # numpy array for the g channel\n",
    "                          image_b=image[bgr[0]].array,  # numpy array for the b channel\n",
    "                          stretch=stretch, Q=Q)  # parameters used to stretch and scale the pixel values\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    # Exposure.getBBox() returns a Box2I, a box with integer pixel coordinates that correspond to the centers of pixels.\n",
    "    # Matplotlib's `extent` argument expects to receive the coordinates of the edges of pixels, which is what\n",
    "    # this Box2D (a box with floating-point coordinates) represents.\n",
    "    integerPixelBBox = image[bgr[0]].getBBox()\n",
    "    bbox = Box2D(integerPixelBBox)\n",
    "    ax.imshow(rgb, interpolation='nearest', origin='lower', extent=(bbox.getMinX(), bbox.getMaxX(), bbox.getMinY(), bbox.getMaxY()))\n",
    "    if fp is not None:\n",
    "        for peak in fp.getPeaks():\n",
    "            ax.plot(peak.getIx(), peak.getIy(), \"bx\", mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we can slice `MultibandImage` objects (as well a `MultibandExposure` objects) along the filter dimension using the filter names as indices. Like `Exposure` objects, `MultibandExposure` objects have `image`, `mask`, and `variance` properties that contain the image, mask plane, and variance of the `Exposure` respectively. For now we will only worry about the `image` property, although internal deblending and measurement algorithms make use of all three objects (when available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showRGB(coadds[:\"z\"].image, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showRGB(coadds[\"i\":].image, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those images are a full \"patch\", which is our usual unit of processing for coadds - it's about the same size as a single LSST sensor.  That's a bit unweildy (just because waiting for processing to happen isn't fun in a tutorial setting), so we'll reload our dict with sub-images centered on the region of interest.\n",
    "\n",
    "Note that we can load the sub-images directly with the `butler`, by appending `_sub` to the DatasetType and passing a `bbox` argument. Use `sampleBBox` to select a sub-region of the image (note that we add a small frame around each blend to include more background regions, which are important for the detection algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 50\n",
    "sampleBBox = Box2I(Point2I(18380-frame, 17750-frame), Extent2I(63+2*frame, 87+2*frame))\n",
    "\n",
    "subset = coadds[:, sampleBBox]\n",
    "# Due to a bug in the code the PSF isn't copied properly.\n",
    "# The code below copies the PSF into the `MultibandExposure`,\n",
    "# but will be unecessary in the future\n",
    "for f in subset.filters:\n",
    "    subset[f].setPsf(coadds[f].getPsf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showRGB(subset.image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Processing\n",
    "\n",
    "Now we'll try the regular LSST processing tasks, with a simpler configuration than we usually use to process coadds, just to avoid being distracted by complexity.  This includes\n",
    "\n",
    "1. Detection (`SourceDetectionTask`): given an `Exposure`, find above-threshold regions and peaks within them (`Footprints`), and create a *parent* source for each `Footprint`.\n",
    "2. Deblending (`ScarletDeblendTask`): given a `MultibandExposure` and a catalog of parent sources, create a *child* source for each peak in every `Footprint` that contains more than one peak.  Each child source is given a `HeavyFootprint`, which contains both the pixel region that source covers and the fractional pixel values associated with that source. A `SourceDeblendTask` is also available using the single band SDSS-HSC deblender that takes a single band `Exposure`).\n",
    "3. Measurment (`SingleFrameMeasurementTask`): given an `Exposure` and a catalog of sources, run a set of \"measurement plugins\" on each source, using deblended pixel values if it is a child. Notice that measurement is still performed on single band catalogs, since none of the measurement algorithms work for multiband data.\n",
    "\n",
    "We'll start by importing these, along with the `SourceCatalog` class we'll use to hold the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.meas.algorithms import SourceDetectionTask\n",
    "from lsst.meas.extensions.scarlet import ScarletDeblendTask\n",
    "from lsst.meas.base import SingleFrameMeasurementTask\n",
    "from lsst.afw.table import SourceCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now construct all of these `Tasks` before actually running any of them.  That's because `SourceDeblendTask` and `SingleFrameMeasurementTask` are constructed with a `Schema` object that records what fields they'll produce, and they modify that schema when they're constructed by adding columns to it.  When we run the tasks later, they'll need to be given a catalog that includes all of those columns, **but we can't add columns to a catalog that already exists**.\n",
    "\n",
    "To recap, the sequence looks like this:\n",
    "\n",
    " 1. Make a (mostly) empty schema.\n",
    " 2. Construct all of the `Task`s (in the order you plan to run them), which adds columns to the schema.\n",
    " 3. Make a `SourceCatalog` object from the *complete* schema.\n",
    " 4. Pass the same `SourceCatalog` object to each `Task` when you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = SourceCatalog.Table.makeMinimalSchema()\n",
    "\n",
    "detectionTask = SourceDetectionTask(schema=schema)\n",
    "\n",
    "config = ScarletDeblendTask.ConfigClass()\n",
    "config.maxIter = 100\n",
    "deblendTask = ScarletDeblendTask(schema=schema, config=config)\n",
    "\n",
    "# We'll customize the configuration of measurement to just run a few plugins.\n",
    "# The default list of plugins is much longer (and hence slower).\n",
    "measureConfig = SingleFrameMeasurementTask.ConfigClass()\n",
    "measureConfig.plugins.names = [\"base_SdssCentroid\", \"base_PsfFlux\", \"base_SkyCoord\"]\n",
    "# \"Slots\" are aliases that provide easy access to certain plugins.\n",
    "# Because we're not running the plugin these slots refer to by default,\n",
    "# we need to disable them in the configuration.\n",
    "measureConfig.slots.apFlux = None\n",
    "#measureConfig.slots.instFlux = None\n",
    "measureConfig.slots.shape = None\n",
    "measureConfig.slots.modelFlux = None\n",
    "measureConfig.slots.calibFlux = None\n",
    "measureConfig.slots.gaussianFlux = None\n",
    "measureTask = SingleFrameMeasurementTask(config=measureConfig, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we'll run is detection, which actually returns a new `SourceCatalog` object rather than working on an existing one.\n",
    "\n",
    "Instead, it takes a `Table` object, which is sort of like a factory for records.  We won't use it directly after this, and it isn't actually necessary to make a new `Table` every time you run `MultibandDetectionTask` (but you can only create one after you're done adding columns to the schema).\n",
    "\n",
    "`Task`s that return anything do so via a `lsst.pipe.base.Struct` object, which is just a simple collection of named attributes.  The only return values we're  interested is `sources`.  That's our new `SourceCatalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = SourceCatalog.Table.make(schema)\n",
    "detectionResult = detectionTask.run(table, subset[\"r\"])\n",
    "catalog = detectionResult.sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at what's in that catalog.  First off, we can look at its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this includes a lot of columns that were actually added by the deblend or measurement steps; those will all still be blank (`0` for integers or flags, `NaN` for floating-point columns).\n",
    "\n",
    "In fact, the only columns filled by `SourceDetectionTask` are the IDs.  But it also attaches `Footprint` objects, which don't appear in the schema.  You can retrieve the `Footprint` by calling `getFootprint()` on a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint = catalog[0].getFootprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Footprints` have two components:\n",
    " - a `SpanSet`, which represents an irregular region on an image via a list of (y, x0, x1) `Spans`;\n",
    " - a `PeakCatalog`, a slightly different kind of catalog whose rows represent peaks within that `Footprint`.\n",
    " \n",
    "You can find a Stack Club notebook devoted to `Footprints` at [SourceDetection/Footprints.ipynb](../SourceDetection/Footprints.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(footprint.getSpans())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(footprint.getPeaks())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we actually look at the footprints in the catalog we see that some have only a single peak, while others have multiple peaks that need to be deblended.\n",
    "\n",
    "To display only the pixels contained in the footprint (and not other pixels in the bounding box) we create a `MultibandFootprint`, which is a `HeavyFootprint` that contains a `SpanSet`, `PeakCatalog`, and `flux` values for all of the pixels in the `SpanSet`. In this case the `flux` is the total measured flux in the image, since no deblending has taken place yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,src in enumerate(catalog):\n",
    "    fp = src.getFootprint()\n",
    "    img = coadds[:,fp.getBBox()].image\n",
    "    mfp = MultibandFootprint.fromImages(coadds.filters, image=img, footprint=fp)\n",
    "    showRGB(mfp.getImage().image, fp=fp, figsize=(3,3))\n",
    "    plt.annotate(str(i), (0.9,0.9), xycoords='axes fraction', color='w', weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that while the peaks *can* have both an integer-valued position and a floating-point position, they're the same right now; `SourceDetectionTask` currently just finds the pixels that are local minima and doesn't try to find their sub-pixel locations.  That's left to the centroider, which is part of the measurement stage.\n",
    "\n",
    "Before we can get to that point, we need to run the deblender:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(deblendTask.run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templateCatalog = deblendTask.run(coadds, catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ScarletDeblendTask` returns a `templateCatalog` that contains the model outputs including heavy footprints that are created.\n",
    "\n",
    "The deblender itself sets the `parent` column for each source, which is `0` for objects with no parent, and all of the columns that begin with `deblend_` and also adds new rows to the catalog for each child. It does *not* remove the parent rows it created those child rows from, and this is intentional, because we want to measure both \"interpretations\" of the blend family: one in which there is only one object (the parent version) and one in which there are several (the children). Before doing any science with the outputs of an LSST catalog, it's important to remove one of those interpretations (typically the parent one).  That can be done by looking at the `deblend_nChild` and `parent` fields:\n",
    "\n",
    "1. `parent` is the ID of the source from which this was deblended, or `0` if the source is itself a parent.\n",
    "2. `deblend_nChild` is the number of child sources this source has (so it's `0` for sources that are themselves children or were never blended).\n",
    " \n",
    "Together, these define two particularly useful filters:\n",
    "\n",
    "1. `deblend_nChild == 0`: never-blended object or de-blended child\n",
    "2. `deblend_nChild == 0 and parent == 0`: never-blended object\n",
    " \n",
    "The first is what you'll usually want to use; the second is what to use if you're willing to throw away some objects (possibly many) because you don't trust the deblender.\n",
    "\n",
    "The last processing step for our purposes is running measurement, which must be done on each catalog, in each band (if we want measurements for all of them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measureTask.run(templateCatalog[\"r\"], coadds['r'])\n",
    "measureTask.run(templateCatalog[\"i\"], coadds['i'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to a feature in the deblender task, the resulting catalogs are not contiguous, and we need to copy them into new objects to use them appropriately. This step can be avoided in the near future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.table as afwTable\n",
    "\n",
    "for f in filters:\n",
    "    _catalog = afwTable.SourceCatalog(templateCatalog[f].table.clone())\n",
    "    _catalog.extend(templateCatalog[f], deep=True)\n",
    "    templateCatalog[f] = _catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we care about deblending (for the sake of this tutorial) lets look at the results from one of the blends displayed above (chosen by hand and specified by `idx`). We use the `HeavyFootprint`s from the catalog sources that have the same parent to build a model for the entire scene, and to compare the results of the flux conserved and *scarlet* models. In the process we look at both the *scarlet* and flux conserved models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.afw.detection import MultibandFootprint\n",
    "from lsst.afw.image import MultibandImage\n",
    "\n",
    "# Note: this is not the parent ID, but the index of the source in the catalog\n",
    "# This index was chosen by hand, and was configured earlier in the notebook\n",
    "parentIdx = idx\n",
    "\n",
    "# Create empty multiband images to model the entire scene\n",
    "templateModel = MultibandImage.fromImages(coadds.filters,\n",
    "                                          [Image(templateCatalog[\"r\"][parentIdx].getFootprint().getBBox(), dtype=np.float32)\n",
    "                                           for b in range(len(filters))])\n",
    "\n",
    "# Only use the subset catalogs with the same parent\n",
    "parentId = templateCatalog[\"r\"][parentIdx].get(\"id\")\n",
    "templateChildren = {b: templateCatalog[b][templateCatalog[b].get(\"parent\")==parentId] for b in filters}\n",
    "\n",
    "for n in range(len(templateChildren[\"r\"])):\n",
    "    # Add the source model to the model of the entire scene\n",
    "    fp = MultibandFootprint(coadds.filters, [templateChildren[b][n].getFootprint() for b in filters])\n",
    "    templateModel[:, fp.getBBox()].array += fp.getImage(fill=0).image.array\n",
    "\n",
    "    # Show the model\n",
    "    showRGB(fp.getImage().image)\n",
    "    plt.show()\n",
    "\n",
    "templateResidual = MultibandImage(coadds.filters,\n",
    "                                  coadds[:, templateModel.getBBox()].image.array - templateModel.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we look at the full models and the residuals. As expected, there are no residuals for the flux conserved model since all of the flux in the image (that is within the footprint) is added to one of the sources. In this particular case that works fine, but in instances where one or more sources were not detected this can cause one source to have its flux contaminated with its neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = [fig.add_subplot(1,2,n+1) for n in range(2)]\n",
    "showRGB(coadds[:,fp.getBBox()].image, ax=ax[0])\n",
    "showRGB(templateModel, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Select some other`sampleBBox` regions and run through the code again, from source detection through measurment and blending displays. Don't foget to change the parent index to view only the children of the correct blend.\n",
    "2. Look at the configuration options on https://github.com/lsst/meas_extensions_scarlet/blob/master/python/lsst/meas/extensions/scarlet/deblend.py and modify some of them to see the changes in results. For example, set `config.symmetric=False` to turn off symmetry to see the effect of deblending without symmetry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
