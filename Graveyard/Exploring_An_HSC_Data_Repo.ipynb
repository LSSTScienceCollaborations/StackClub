{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a Data Repository\n",
    "\n",
    "<br>Owner: **Rob Morgan** ([@rmorgan10](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@rmorgan10)), **Phil Marshall** ([@drphilmarshall](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@drphilmarshall)), **Alex Drlica-Wagner** ([@kadrlica](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@kadrlica))\n",
    "<br>Some minor updates by:   Douglas Tucker ([@douglasleetucker](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@douglasleetucker))\n",
    "<br>Last Verified to Run: **2021-03-12**\n",
    "<br>Verified Stack Release: **v21.0.0**\n",
    "\n",
    "This notebook examines the content of a data repository -- in this particular case, an HSC data repository -- and shows how to determine the inputs for each component. A related notebook is \"Exploring a DC2 Data Repository,\" which explores the same basic concepts, but using a DC2 data repository; this other notebook covers matters in a slightly differ manner, though, and it is useful to explore both notebooks for a fuller understanding of data repositories.\n",
    "\n",
    "### Learning Objectives:\n",
    "After working through and studying this notebook you should be able to understand how to use the Butler to figure out: \n",
    "   1. What a data repo is;\n",
    "   2. Which data types are present in a data repository;\n",
    "   3. If coadds have been made, what the available tracts are;\n",
    "   4. Which parts of the sky those tracts cover.\n",
    "   \n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lsp-stable.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess as sub\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import os, glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Repo?\n",
    "\n",
    "A data repo is a directory containing raw images, calibration files, metadata and configuration information, defining an LSST-format dataset. Data repositories contain either a `_mapper` file or a `repositoryCfg.yaml` file, which record the \"obs package\" that was used to organize the data. The obs package gives the repository more structure and organization than an ordinary data directory. Let's take a look at this file structure in the HSC data repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The HSC Data Repo: What's in there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `hsc` data repository as our testing ground, and start by figuring out what it contains. In the `hsc` case, the `_mapper` file is in the top level folder, while the data repo for each field is a few levels down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/datasets/hsc/repo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `_mapper` file here, and at contains one line giving the name of the `Mapper` object for the HSC repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/_mapper\n",
    "\n",
    "# Import the Mapper object once you know its name\n",
    "from lsst.obs.hsc import HscMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some more information on this object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(HscMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper defines a (large) number of different \"dataset types\". Some of these are specific to this particular data repo, others are more general. Even filtering out some intermediate dataset types, we are still left with a long list. But, once we figure out which dataset types we are interested in, we can start querying for information about those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = HscMapper(root=repo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Butler`, directed by the `Mapper`, will have access to all the above dataset types. \n",
    "\n",
    "Another important file in the repo parent folder is `registry.sqlite3`. This database contains metadata for the HSC **raw** images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, but where is the actual data, and how was it processed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw visit images are stored by field. In the HSC dataset the fields have names like `COSMOS` and `DEEPE09`. Within those field folders, there is a directory structure that eventually gets down to visit image FITS files whose names and paths contain the date/time and filter for that exposure. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! \\ls /datasets/hsc/repo/COSMOS/2015-01-18/01113/HSC-Y/HSC-0018476-00?.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the pipeline tasks know which raw data to process? This information is captured in the \"configs\". In the HSC repo there is no config folder or files in the top level directory - in fact the only two files are `_mapper` and `registry.sqlite3`. So what's going on? \n",
    "\n",
    "It turns out that the provenance of the stack processing of the HSC raw images is captured in  \"rerun\" folders, one for each time the science pipelines were run on the data.  Let's do some detective work to find out what happened to the HSC data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, `DM-10404` looks like a run ID. What's in that folder? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DEEP`, `WIDE`, and `UDEEP` are the names of the sub-surveys of the HSC survey. We might expect each to contain results from the processing of that sub-survey's images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/UDEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerically-named folders contain the generated catalog files, organized by sky tract. Note that a `config` folder is present, and also a `repositoryCfg.yaml` file - which means that this folder is itself a `repo`, from the Butler's point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/WIDE/repositoryCfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean that `SFM` is the \"parent\" of this repo? Let's see what _that_ folder contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SFM` seems to contain _all_ the tracts that have been produced - so is the entire HSC survey. It's `repositoryCfg.yaml` file shows that it's \"parent\" is the top level folder, `/datasets/hsc/repo/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/SFM/repositoryCfg.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the `DM-10404/UDEEP` repo's `config` folder contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -a /datasets/hsc/repo/rerun/DM-10404/UDEEP/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the configuration files that were used when the science pipelines were run on these data. While we don't know which versions of the software were used, we at least know which tasks were run. \n",
    "\n",
    "Here's what a config file looks like (ignoring the many import statements and just looking at a few example lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /datasets/hsc/repo/rerun/DM-10404/UDEEP/config/forcedPhotCcd.py | grep -v import | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next level to dig into here is the tract folders within one of the repos in this rerun. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /datasets/hsc/repo/rerun/DM-10404/UDEEP/00814/HSC-Y/tract9570 | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those FITS files contain the forced source tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the Butler and looking for Dataset Types\n",
    "\n",
    "Now that we have an idea of the structure of the repo itself, let's use the Butler to explore the data within the repo. Here we will demonstrate a few useful `Butler` methods for learning about the data in a repo. Let's choose one of the rerun repos, and investigate its properties. We'll summon two butlers, one that is pointed at the parent repo, and another (an \"under butler\") that is asked to focus on a particular sub-survey in a particular re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_repo = '/datasets/hsc/repo'\n",
    "\n",
    "# Choose a re-run repo:\n",
    "rerun_id = 'DM-10404'\n",
    "depth = 'UDEEP'\n",
    "# Try a different one:\n",
    "# rerun = 'DM-13666'\n",
    "# depth = 'WIDE'\n",
    "\n",
    "repo = parent_repo + '/rerun/' + rerun_id + '/' + depth\n",
    "print(repo)\n",
    "\n",
    "from lsst.daf.persistence import Butler\n",
    "\n",
    "butler = Butler(parent_repo)\n",
    "under_butler = Butler(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `butler` can check whether a datatype (like the source catalogs) actually exists or not, but it needs a specific dataset ID to check whether that specific part of the dataset exists. \n",
    "\n",
    "Note that the metadata being queried here is in the `registry.sqlite3` database in the _parent_ repo - and so refers to the _initial_ processing run, not the most recent rerun. We'll need to work carefully around this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Basic Dataset Properties Using the Butler\n",
    "Now we can start using Butler methods to query metadata for the repo. For this dataset, we can look at the filters used, number of visits, number of pointings, etc. by examining the Butler's keys and metadata. For these basic properties, we will look at the `calexp` and `src` tables. The contents of these tables are derived from the processing of individual sensors, and exist in the parent folder. (That means that we can use either of our two butlers to query for them.)\n",
    "\n",
    "Note that the metadata is created from the raw exposures loaded into the sqlite registry. The fact that we can get metadata for a specific datasetType and dataId **does not** imply that the data exist on disk (we will check this in a subsequent step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be faster if only one query were issued...\n",
    "visits = butler.queryMetadata('calexp', ['visit'])\n",
    "pointings = butler.queryMetadata('calexp', ['pointing'])\n",
    "ccds = butler.queryMetadata('calexp', ['ccd'])\n",
    "fields = butler.queryMetadata('calexp', ['field'])\n",
    "filters = butler.queryMetadata('calexp', ['filter'])\n",
    "sources = butler.queryMetadata('src', ['id'])\n",
    "\n",
    "# It is possible to specify multiple formats -- i.e., butler.queryMetadata('calexp', ['visit','ccd'])\n",
    "metadata = butler.queryMetadata('calexp', ['visit','pointing','ccd','field','filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visits = len(visits)\n",
    "num_pointings = len(pointings)\n",
    "num_ccds = len(ccds)\n",
    "num_fields = len(fields)\n",
    "num_filters = len(filters)\n",
    "num_sources = len(sources)\n",
    "num_metadata = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The HSC {}/{} rerun contains {} visits.\".format(rerun_id,depth,num_visits))\n",
    "print(\"The HSC {}/{} rerun contains {} pointings.\".format(rerun_id,depth,num_pointings))\n",
    "print(\"The HSC {}/{} rerun contains {} ccds.\".format(rerun_id,depth,num_ccds))\n",
    "print(\"The HSC {}/{} rerun contains {} fields.\".format(rerun_id,depth,num_fields))\n",
    "print(\"The HSC {}/{} rerun contains {} filters.\".format(rerun_id,depth,num_filters))\n",
    "print(\"The HSC {}/{} rerun contains {} sources.\".format(rerun_id,depth,num_sources))\n",
    "# print(\"The HSC {}/{} rerun contains {} coadd sources.\".format(rerun_id,depth,num_coadd_sources))\n",
    "# print(\"The HSC {}/{} rerun contains {} forced sources.\".format(rerun_id,depth,num_forced_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, let's compare what our two butlers find when asked for the number of sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_num_sources = len(under_butler.queryMetadata('src', ['id']))\n",
    "\n",
    "print(\"The butler says that we have {:d} input sources.\".format(num_sources))\n",
    "print(\"The under butler says we have {:d} input sources.\".format(alt_num_sources))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we could have done our query with the `under_butler` as well. In practice, it's best to specify a Butler for the rerun repo, because that Butler will also have access to the parent repo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So where are the Calexps and Source Catalogs?\n",
    "\n",
    "Notice that while we were able to get metadata for the `processCcd` outputs (the `calexp` and `src`), that **does not** guarantee that these products are on disk. The metadata is created from the raw inputs stored in the registry and a template for the derived data products. In this case, we have a reason to be suspicious: remember that when we examined the directory structure we did not see a directory for the `processCcd` products. \n",
    "\n",
    "To check the existence of the data requires the use of the `datasetExists` method of the `butler`. Let's give this a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select the metadata for a specific calexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId={'filter':'HSC-I','visit':872,'ccd':10}\n",
    "butler.queryMetadata('calexp', ['visit','ccd','filter','field','pointing'], dataId=dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have the metadata, let's try to get the calexp..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    calexp = butler.get('calexp',dataId=dataId)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    print(\"\\nWhat? Does the data exist?\")\n",
    "\n",
    "# Explicitly check for existence\n",
    "exists = butler.datasetExists('calexp',dataId=dataId)\n",
    "print(\"\\nbutler.datasetExists: \" + str(exists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears that we don't have these intermediate products stored in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coadd Sky Area\n",
    "\n",
    "One may also be interested in the total sky area imaged for a particular coadd rerun/depth. We can estimate and visualize this from the coadd tract info that neither our `under_butler` nor our `butler` has access to. To collect all the tracts, we have to get them via the file structure. This operation will hopefully be `Butler`-ized with the Gen3 Butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tract indices from file names\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(repo, 'deepCoadd-results', 'merged', '*'))])\n",
    "num_tracts = len(tracts)\n",
    "\n",
    "print(\"Found {} merged tracts in repo {}\".format(num_tracts, repo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of extimating the sky area covered is to sum the areas of the inner boxes of all the tracts. For more information on the properties of tracts, you can look at the [Documentation](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1skymap_1_1tract_info_1_1_tract_info.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, the file structure only tells us the names of the tracts in the particular rerun/depth to look at. The actual `TractInfo` objects are obtained by selecting the tracts we want from the `deepCoadd_skyMap` dataset in our particular rerun repo. Therefore, we will have to ask the `under_butler` to bring us this dataset for the particular rerun/depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area from all tracts\n",
    "skyMap = under_butler.get('deepCoadd_skyMap')\n",
    "total_area = 0.0  #deg^2\n",
    "plotting_vertices = []\n",
    "for test_tract in tracts:\n",
    "    # Get inner vertices for tract\n",
    "    tractInfo = skyMap[test_tract]\n",
    "    vertices = tractInfo._vertexCoordList\n",
    "    plotting_vertices.append(vertices)\n",
    "    \n",
    "    #calculate area of box\n",
    "    av_dec = 0.5 * (vertices[2][1] + vertices[0][1])\n",
    "    av_dec = av_dec.asRadians()\n",
    "    delta_ra_raw = vertices[0][0] - vertices[1][0] \n",
    "    delta_ra = delta_ra_raw.asDegrees() * np.cos(av_dec)\n",
    "    delta_dec= vertices[2][1] - vertices[0][1]\n",
    "    area = delta_ra * delta_dec.asDegrees()\n",
    "    \n",
    "    #combine areas\n",
    "    total_area += area\n",
    "    \n",
    "\n",
    "# Round off the total area for presentation purposes\n",
    "rounded_total_area = round(total_area, 2)\n",
    "\n",
    "print(\"Total area imaged (sq deg): \",rounded_total_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Dataset Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out a report of all the characteristcs we have found. We'll use the sky area from the rerun we chose, and the numbers common to all reruns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HSC'\n",
    "display(Markdown('### %s' % repo))\n",
    "\n",
    "# Make a table of the collected metadata\n",
    "collected_data = [num_visits, num_pointings, num_ccds, num_fields, num_filters, num_sources, \n",
    "                  num_tracts, rounded_total_area]\n",
    "data_names = (\"Number of Visits\", \"Number of Pointings\", \"Number of CCDs\", \"Number of Fields\", \n",
    "              \"Number of Filters\", \"Number of Sources\", \"Number of Tracts\", \"Total Sky Area (deg$^2$)\")\n",
    "# TODO: include coadd sources and forced sources\n",
    "\n",
    "output_table = \"|   Metadata Characteristic  | Value | \\n  | ---: | ---: | \\n \"\n",
    "counter = 0\n",
    "while counter < len(collected_data):\n",
    "    output_table += \"| %s |  %s | \\n\" %(data_names[counter], collected_data[counter])\n",
    "    counter += 1\n",
    "display(Markdown(output_table))\n",
    "\n",
    "# Show which fields and filters we're talking about:\n",
    "display(Markdown('Fields: (%i total)' %num_fields))\n",
    "print(fields)\n",
    "display(Markdown('Filters: (%i total)' %num_filters))\n",
    "print(filters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the sky coverage\n",
    "\n",
    "For this we will need our list of merged `tracts` from above, and also the `skyMap` object. We can then extract the sky coordinates of the corners of each tract, and use them to draw a set of rectangles to illustrate the sky coverage, following Jim Chiang's LSST DESC tutorial [dm_butler_skymap.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/dm_butler_skymap.ipynb).\n",
    "\n",
    "In the future, we could imagine overlaying the focal plane and color the individual visits, using more of the code from Jim's notebook. Let's wait to see what functionality the Gen3 Butler provides first, and maybe return to visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for tract in tracts:\n",
    "    tractInfo = skyMap[tract]\n",
    "        \n",
    "    corners = [(x[0].asDegrees(), x[1].asDegrees()) for x in tractInfo.getVertexList()]\n",
    "    x = [k[0] for k in corners] + [corners[0][0]]\n",
    "    y = [k[1] for k in corners] + [corners[0][1]]\n",
    "    \n",
    "       \n",
    "    plt.plot(x,y, color='b')\n",
    "    \n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('Dec (deg)')\n",
    "plt.title('2D Projection of Sky Coverage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could imagine plotting the patches as well, to show which tracts were incomplete - but this gives us a rough idea of where our data is on the sky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Coadd Object Catalog\n",
    "\n",
    "Since we know we have `deepCoadd` images, we might want to find the sources detected in those images. These are the precursors of `Objects`. If forced photometry has been run on these sources, there should be `deepCoadd_forced_src` catalogs present. The `config` tells us which tasks have been run in the rerun, but metadata describing the results of the run is not propagated back into the registry database in the parent folder. So, we need to get data from the `under_butler`.\n",
    "\n",
    "Since the registry only knows about the raw exposures (and products that can be directly derived from templates using the raws), we can't get valid metadata from `queryMetadata` directly. Instead, we query the directory structure for a valid filter, tract, patch combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname=os.path.join(repo,'deepCoadd-results')\n",
    "print('filters: '+dirname)\n",
    "!ls $dirname\n",
    "filter='HSC-I'\n",
    "\n",
    "dirname = os.path.join(dirname, filter)\n",
    "print('\\ntracts: '+dirname)\n",
    "!ls $dirname\n",
    "tract=8523\n",
    "\n",
    "dirname = os.path.join(dirname,str(tract))\n",
    "print('\\npatches: '+dirname)\n",
    "!ls $dirname\n",
    "patch='1,3'\n",
    "\n",
    "dirname = os.path.join(dirname,patch)\n",
    "print('\\ndata products: '+dirname)\n",
    "!ls $dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some available tract, patches from the `skyMap` object. We need to feed it a list of coordinates defining a polygon to search for viable tract,patch info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.geom import SpherePoint, Angle\n",
    "# Create a list of corners of the polygon to search.\n",
    "coordList = [SpherePoint(Angle(np.radians(0)),Angle(np.radians(-6))),\n",
    "             SpherePoint(Angle(np.radians(0)),Angle(np.radians(-2))),\n",
    "             SpherePoint(Angle(np.radians(40)),Angle(np.radians(-2))),\n",
    "             SpherePoint(Angle(np.radians(40)),Angle(np.radians(-6)))\n",
    "            ]\n",
    "# Print the tract,patch info\n",
    "tractInfo = skyMap.findTractPatchList(coordList)\n",
    "for _tract in tractInfo:\n",
    "    print(_tract[0])\n",
    "    for _patch in _tract[1]:\n",
    "        print('  ',_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we grab the coadd source catalog for a specific filter\n",
    "dataId={'filter':'HSC-I','tract':tract,'patch':patch}\n",
    "coadd_sources = under_butler.get('deepCoadd_forced_src',dataId=dataId)\n",
    "coadd_sources.asAstropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the merged detections\n",
    "dataId={'tract':8523,'patch':'1,8'}\n",
    "merged_sources = under_butler.get('deepCoadd_mergeDet',dataId=dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One can get the available keys by running : \n",
    "#coadd_sources.getSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Simple Measurements\n",
    "\n",
    "We'd like to know something about the sources that have been detected and measured, beyond a simple total number. We create a simple histogram of the Kron flux of sources in our HSC-I band tile. You can find more details on merging catalogs across bands, etc on the [Science Pipelines documentation](https://pipelines.lsst.io/getting-started/multiband-analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(coadd_sources['ext_photometryKron_KronFlux_flux'],bins=np.linspace(0,100,50))\n",
    "plt.xlabel(\"log10(Kron Flux)\")\n",
    "plt.title(\"{filter} {tract} {patch}\".format(filter=filter,tract=tract,patch=patch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have shown a few techniques for exploring a data repo. ~~To make this process straightforward, we are implementing all these techniques into methods of a `Taster` class, which is now a part of the `stackclub` library. The `Taster` will give you a taste of what the `Butler` delivers. we demonstrate the use of this class in the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/dc2_gen2/Graveyard/DataInventory.ipynb) notebook.~~\n",
    "\n",
    "(Note:  the `Taster` class and the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/dc2_gen2/Graveyard/DataInventory.ipynb) notebook are now both deprecated.  The DataInventory notebook is largely a demonstration of the `Taster` class, a useful tool that was created to augment the deficiencies of the Gen-2 butler for investigating and exploring data repos. Unfortunately, it depends on classes and repo structures that have evolved substantially over the past few years. Since the `Taster` resides outside of the DM Stack, it requires constant maintenance to be kept up-to-date with the Stack.  With the impending release of the Gen-3 butler, the `Taster` and the DataInventory notebook have been deprecated.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
