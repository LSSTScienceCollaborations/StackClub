{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring a DC2 Data Repository\n",
    "\n",
    "<br>Owner: **Rob Morgan** ([@rmorgan10](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@rmorgan10)), **Phil Marshall** ([@drphilmarshall](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@drphilmarshall)), **Alex Drlica-Wagner** ([@kadrlica](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@kadrlica))\n",
    "<br>Updated for DC2 by:   Douglas Tucker ([@douglasleetucker](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@douglasleetucker)), basically copying the DC2-updated notebook created by **Johann Cohen-Tanugi** ([@johannct](https://github.com/johannct)) in the DESC fork of the Stack Club GitHub repo (https://github.com/LSSTDESC/StackClub).\n",
    "<br>Last Verified to Run: **2021-03-11**\n",
    "<br>Verified Stack Release: **v21.0.0**\n",
    "\n",
    "This notebook examines the content of a data repository -- in this particular case, a DC2 data repository -- and shows how to determine the inputs for each component.  A related notebook is \"Exploring an HSC Data Repository,\" which explores the same basic concepts, but using an HSC data repository; this other notebook covers matters in a slightly differ manner, though, and it is useful to explore both notebooks for a fuller understanding of data repositories.\n",
    "\n",
    "### Learning Objectives:\n",
    "After working through and studying this notebook you should be able to understand how to use the Butler to figure out: \n",
    "   1. What a data repo is;\n",
    "   2. Which data types are present in a data repository;\n",
    "   3. If coadds have been made, what the available tracts are;\n",
    "   4. Which parts of the sky those tracts cover.\n",
    "   \n",
    "### Logistics\n",
    "This notebook is intended to be runnable on `lsst-lsp-stable.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub.\n",
    "\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess as sub\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np\n",
    "import os, glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access DC2 calexp gen2 repository\n",
    "from lsst.daf.persistence import Butler\n",
    "repo = '/datasets/DC2/DR6/Run2.2i/patched/2021-02-10/rerun/run2.2i-coadd-wfd-dr6-v1'\n",
    "butler = Butler(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Data Repo?\n",
    "\n",
    "A data repo is a directory containing raw images, calibration files, metadata and configuration information, defining an LSST-format dataset. Data repositories contain either a `_mapper` file or a `repositoryCfg.yaml` file, which record the \"obs package\" that was used to organize the data. The obs package gives the repository more structure and organization than an ordinary data directory. Let's take a look at this file structure in the DC2 data repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DC2 Data Repo: What's in there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dc2 data repository as our testing ground. As a production directory, it contains a lot of files presiding to the actual production pipelines, and not of interest here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the top-level dir of the DC2 repo\n",
    "topdir = os.path.join(repo,'../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of `topdir`\n",
    "out=sub.check_output (['ls', topdir])\n",
    "print(out.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important files and directories are the following :\n",
    "\n",
    " - **raw** directory : it contains all the raw files ingested by the butler and registered into the registry.sqlite3 database.\n",
    " - **CALIB** and **calibrations** : these hold calibration products that the Rubin science pipelines use to process the raw images.\n",
    " - **ref_cats** : a directory containing the reference catalogue(s) for astrometry and photometry calibrations.\n",
    " - **rerun** : the directory, built by the butler, that will contain all the processing outputs of the science pipelines : calibrated exposures, coadded images and catalogues, etc...\n",
    " - **_mapper** : a file that indicates to the butler what instrument is supposed to be considered for the processing task \n",
    " \n",
    "Let's have a further look at the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=sub.check_output (['cat', os.path.join(topdir,'_mapper')])\n",
    "print(out.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the `_mapper` file here, and it contains one line giving the name of the Mapper object for the DC2 repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Mapper object once you know its name\n",
    "from lsst.obs.lsst.imsim import ImsimMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get some more information on this object like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ImsimMapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper defines a (large) number of different \"dataset types\". Some of these are specific to this particular data repo, others are more general. Even filtering out some intermediate dataset types, we are still left with a long list. But, once we figure out which dataset types we are interested in, we can start querying for information about those datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = ImsimMapper(root=repo)\n",
    "all_dataset_types = mapper.getDatasetTypes()\n",
    "\n",
    "remove = ['_config', '_filename', '_md', '_sub', '_len', '_schema', '_metadata']\n",
    "\n",
    "shortlist = []\n",
    "for dataset_type in all_dataset_types:\n",
    "    keep = True\n",
    "    for word in remove:\n",
    "        if word in dataset_type:\n",
    "            keep = False\n",
    "    if keep:\n",
    "        shortlist.append(dataset_type)\n",
    "\n",
    "print(shortlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Butler`, directed by the `Mapper`, will have access to all the above dataset types. \n",
    "\n",
    "Another important file in the repo parent folder is `registry.sqlite3`. This database contains metadata for the **raw** images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great, but where is the actual data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found where the raw images are, in the topdir, but the actual processing outputs are actually two level down, inside the **rerun** directory. Under **rerun** several directories correspond to dedicated parts of latest versions of the processing of the 2.2i simulations. The one we will focus on here is provided by the varaible repo defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=sub.check_output (['ls', repo])\n",
    "print(out.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **repositoryCfg.yaml** file allows the mapper to recognize that this is a data repository, and its content points the butler to additional data repositories, joined to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=sub.check_output(['cat', os.path.join(repo,'repositoryCfg.yaml')])\n",
    "print(out.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can recognize a relative path to the top directory (_root), the mapper content, and three parent data repositories that were chained to the one we are looking at: run2.2i-calexp-v1 contains the outputs of the exposure calibrations (single visit processing), run2.2i-coadd-wfd-dr6-v1-* which contain the outputs of the coaddition processing, while the current repository contains the final outputs, measurements and forced photometry on the coadds, and metacalibration of the shear, under the deepCoadd and deepCoadd-results directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Basic Dataset Properties Using the Butler\n",
    "\n",
    "The butler has been instantiated above, and knows about the data repository structure and chaining, as described previously.\n",
    "\n",
    "Now we can start using Butler methods to query metadata for the repo. For this dataset, we can look at the filters used, number of visits, number of pointings, etc. by examining the Butler's keys and metadata. For these basic properties, we will look at the calexp and src tables. The contents of these tables are derived from the processing of individual sensors, and exist in the parent folder. \n",
    "\n",
    "Note that the metadata is created from the raw exposures loaded into the sqlite registry. The fact that we can get metadata for a specific datasetType and dataId **does not** imply that the data exist on disk (we will check this in a subsequent step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be faster if only one query were issued...\n",
    "visits = butler.queryMetadata('calexp', ['visit'])\n",
    "ccds = butler.queryMetadata('calexp', ['detector'])\n",
    "filters = butler.queryMetadata('calexp', ['filter'])\n",
    "sources = butler.queryMetadata('src', ['id'])\n",
    "\n",
    "# It is possible to specify multiple formats -- i.e., butler.queryMetadata('calexp', ['visit','ccd'])\n",
    "metadata = butler.queryMetadata('calexp', ['visit','detector','filter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_visits = len(visits)\n",
    "num_ccds = len(ccds)\n",
    "num_filters = len(filters)\n",
    "num_sources = len(sources)\n",
    "num_metadata = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This DC2 repo contains {} visits.\".format(num_visits))\n",
    "print(\"This DC2 repo contains {} ccds.\".format(num_ccds))\n",
    "print(\"This DC2 repo contains {} filters.\".format(num_filters))\n",
    "print(\"This DC2 repo contains {} sources.\".format(num_sources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we find out that the butler sees 189 ccd possible entries (there are 189 science detectors in the camera) and 6 filters.... But here are also the grand total of visits and sources (detected on calibrated exposures, not coadded images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So where are the Calexps and Source Catalogs?\n",
    "\n",
    "Notice that while we were able to get metadata for the `processCcd` outputs (the `calexp` and `src`), that **does not** guarantee that these products are on disk. The metadata is created from the raw inputs stored in the registry and a template for the derived data products. \n",
    "\n",
    "To check the existence of the data requires the use of the `datasetExists` method of the `butler`. Let's give this a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select the metadata for a specific calexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId={'filter':'i','visit':665637,'detector':10}\n",
    "butler.queryMetadata('calexp', ['visit','detector','filter'], dataId=dataId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have the metadata, let's try to get the calexp..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    calexp = butler.get('calexp',dataId=dataId)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "    print(\"\\nWhat? Does the data exist?\")\n",
    "\n",
    "# Explicitly check for existence\n",
    "exists = butler.datasetExists('calexp',dataId=dataId)\n",
    "print(\"\\nbutler.datasetExists: \" + str(exists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset exists, and we can decide to inspect the image, or the source catalogue associated with this visit. All this is for another tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on the calexps : visualizing and getting visit information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's have a look at the calexp methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp_methods = [m for m in dir(calexp) if not m.startswith('_')]\n",
    "calexp_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of a calibration exposure is a set of three images : the exposure itself, the mask flagging problematic pixels, and the variance plane. We can visualize these using the Rubin science pipeline's display module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.afw.display as afw_display\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "display1 = afw_display.Display(frame=1, backend='matplotlib')\n",
    "display1.scale(\"linear\", \"zscale\")\n",
    "display1.mtv(calexp.maskedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`maskedImage` shows the same exposure as calexp.image, but with the mask plane superposed. You can visualize `calexp.mask` to see the mask plane, and `calexp.variance` to visualize the noise of the ccd (you may need to comment out the scale command)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WCS and Metadata\n",
    "\n",
    "These are very useful information available once a calexp is retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs = calexp.getWcs()\n",
    "wcs.pixelToSky(100.0, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = calexp.getMetadata()\n",
    "print(metadata.toDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better metadata: ExposureInfo, VisitInfo, PSF, and photometric calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp_info = calexp.getInfo()\n",
    "visit_info = calexp_info.getVisitInfo()\n",
    "visit_info.getWeather()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the DC2 simulation the weather is not interesting....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calexp.hasPsf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf = calexp.getPsf()\n",
    "from lsst.geom import PointD\n",
    "psfimage = psf.computeImage(PointD(100.,100.))\n",
    "display1 = afw_display.Display(frame=1, backend='matplotlib')\n",
    "display1.scale('asinh', min=0.0, max=1.e-3, unit='absolute')\n",
    "display1.mtv(psfimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib = calexp.getPhotoCalib()\n",
    "print(calib)\n",
    "calib.getInstFluxAtZeroMagnitude() #the zero point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coadd Sky Area\n",
    "\n",
    "One may also be interested in the total sky area imaged in a given processing (and thus a given repository). First let's remind ourselves that for the purpose of processing, the sky is tiled by overlapping tracts. Each tract is also divided into overlapping patches. Coaddition occurs at the patch level, while the WCS solution is constructed over the tract. The geometrical sizes of tracts and patches are configurables when defining how the processing of data from a given instrument should proceed. For DC2, there are for instance 49 patches per tract.\n",
    "\n",
    "Let's use the repository's structure to list the available tracts :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect tract indices from file names\n",
    "tracts = sorted([int(os.path.basename(x)) for x in\n",
    "                 glob.glob(os.path.join(repo, 'deepCoadd-results', 'merged', '*'))])\n",
    "num_tracts = len(tracts)\n",
    "\n",
    "print(\"Found {} merged tracts in repo {}\".format(num_tracts, repo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way of extimating the sky area covered is to sum the areas of the inner boxes of all the tracts. For more information on the properties of tracts, you can look at the [Documentation](http://doxygen.lsst.codes/stack/doxygen/x_masterDoxyDoc/classlsst_1_1skymap_1_1tract_info_1_1_tract_info.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, the file structure only tells us the names of the tracts in the particular rerun to look at. The actual `TractInfo` objects are obtained by selecting the tracts we want from the `deepCoadd_skyMap` dataset in our particular rerun repo. Therefore, we will have to ask the `butler` to serve this dataset for the particular rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate area from all tracts\n",
    "skyMap = butler.get('deepCoadd_skyMap')\n",
    "total_area = 0.0  #deg^2\n",
    "plotting_vertices = []\n",
    "for test_tract in tracts:\n",
    "    # Get inner vertices for tract\n",
    "    tractInfo = skyMap[test_tract]\n",
    "    vertices = tractInfo._vertexCoordList\n",
    "    plotting_vertices.append(vertices)\n",
    "    \n",
    "    #calculate area of box\n",
    "    av_dec = 0.5 * (vertices[2][1] + vertices[0][1])\n",
    "    av_dec = av_dec.asRadians()\n",
    "    delta_ra_raw = vertices[0][0] - vertices[1][0] \n",
    "    delta_ra = delta_ra_raw.asDegrees() * np.cos(av_dec)\n",
    "    delta_dec= vertices[2][1] - vertices[0][1]\n",
    "    area = delta_ra * delta_dec.asDegrees()\n",
    "    \n",
    "    #combine areas\n",
    "    total_area += area\n",
    "    \n",
    "\n",
    "# Round off the total area for presentation purposes\n",
    "rounded_total_area = round(total_area, 2)\n",
    "\n",
    "print(\"Total area imaged (sq deg): \",rounded_total_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result differs from the expected 300 sq deg for several reasons : the computation here is a flat approximation, the tracts are actually overlapping, and the DC2 processing extends beyond the strict 300 sq deg region to avoid edge effects within the fiducial region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Dataset Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out a report of all the characteristics we have found above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('### %s' % repo))\n",
    "\n",
    "# Make a table of the collected metadata\n",
    "collected_data = [num_visits, num_ccds, num_filters, num_sources, \n",
    "                  num_tracts, rounded_total_area]\n",
    "data_names = (\"Number of Visits\", \"Number of CCDs\", \n",
    "              \"Number of Filters\", \"Number of Sources\", \"Number of Tracts\", \"Total Sky Area (deg$^2$)\")\n",
    "# TODO: include coadd sources and forced sources\n",
    "\n",
    "output_table = \"|   Metadata Characteristic  | Value | \\n  | ---: | ---: | \\n \"\n",
    "counter = 0\n",
    "while counter < len(collected_data):\n",
    "    output_table += \"| %s |  %s | \\n\" %(data_names[counter], collected_data[counter])\n",
    "    counter += 1\n",
    "display(Markdown(output_table))\n",
    "\n",
    "# Show which filters we're talking about:\n",
    "display(Markdown('Filters: (%i total)' %num_filters))\n",
    "print(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the sky coverage\n",
    "\n",
    "For this we will need our list of merged `tracts` from above, and also the `skyMap` object. We can then extract the sky coordinates of the corners of each tract, and use them to draw a set of rectangles to illustrate the sky coverage, following Jim Chiang's LSST DESC tutorial [dm_butler_skymap.ipynb](https://github.com/LSSTDESC/DC2-analysis/blob/master/tutorials/dm_butler_skymap.ipynb).\n",
    "\n",
    "In the future, we could imagine overlaying the focal plane and color the individual visits, using more of the code from Jim's notebook. Let's wait to see what functionality the Gen3 Butler provides first, and maybe return to visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for tract in tracts:\n",
    "    tractInfo = skyMap[tract]\n",
    "        \n",
    "    corners = [(x[0].asDegrees(), x[1].asDegrees()) for x in tractInfo.getVertexList()]\n",
    "    x = [k[0] for k in corners] + [corners[0][0]]\n",
    "    y = [k[1] for k in corners] + [corners[0][1]]\n",
    "    \n",
    "       \n",
    "    plt.plot(x,y, color='b')\n",
    "    \n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('Dec (deg)')\n",
    "plt.title('2D Projection of Sky Coverage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could imagine plotting the patches as well, to show which tracts were incomplete - but this gives us a rough idea of where our data is on the sky.  More in-depth usage of the skymap can be found in other tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a Coadd Object Catalog\n",
    "\n",
    "Since we know we have `deepCoadd` images, we might want to find the sources detected in those images. These are the precursors of `Objects`. If forced photometry has been run on these sources, there should be `deepCoadd_forced_src` catalogs present. The `config` tells us which tasks have been run in the rerun, but metadata describing the results of the run is not propagated back into the registry database in the parent folder. So, we need to get data from a separate butler which we will call in the following steps the `under_butler` (this is equivalent to the `under_butler` in the \"Exploring An HSC Data Repository\").\n",
    "\n",
    "Since the registry only knows about the raw exposures (and products that can be directly derived from templates using the raws), we can't get valid metadata from `queryMetadata` directly. Instead, we query the directory structure for a valid filter, tract, patch combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname=os.path.join(repo,'deepCoadd-results')\n",
    "print('filters: '+dirname)\n",
    "!ls $dirname\n",
    "filter='i'\n",
    "\n",
    "dirname = os.path.join(dirname, filter)\n",
    "print('\\ntracts: '+dirname)\n",
    "!ls $dirname\n",
    "tract=3828\n",
    "\n",
    "dirname = os.path.join(dirname,str(tract))\n",
    "print('\\npatches: '+dirname)\n",
    "!ls $dirname\n",
    "patch='1,3'\n",
    "\n",
    "dirname = os.path.join(dirname,patch)\n",
    "print('\\ndata products: '+dirname)\n",
    "!ls $dirname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some available tract, patches from the `skyMap` object. We need to feed it a list of coordinates defining a polygon to search for viable tract,patch info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsst.geom import SpherePoint, Angle\n",
    "# Create a list of corners of the polygon to search.\n",
    "coordList = [SpherePoint(Angle(np.radians(60)),Angle(np.radians(-32))),\n",
    "             SpherePoint(Angle(np.radians(60)),Angle(np.radians(-33))),\n",
    "             SpherePoint(Angle(np.radians(61)),Angle(np.radians(-33))),\n",
    "             SpherePoint(Angle(np.radians(61)),Angle(np.radians(-32)))\n",
    "            ]\n",
    "# Print the tract,patch info\n",
    "tractInfo = skyMap.findTractPatchList(coordList)\n",
    "for _tract in tractInfo:\n",
    "    print(_tract[0])\n",
    "    for _patch in _tract[1]:\n",
    "        print('  ',_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we grab the coadd source catalog for a specific filter\n",
    "dataId={'filter':'i','tract':tract,'patch':patch}\n",
    "coadd_sources = butler.get('deepCoadd_forced_src',dataId=dataId)\n",
    "coadd_sources.asAstropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the merged detections\n",
    "dataId={'tract':4434,'patch':'3,3'}\n",
    "merged_sources = butler.get('deepCoadd_mergeDet',dataId=dataId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One can get the available keys by running : \n",
    "#coadd_sources.getSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Simple Measurements\n",
    "\n",
    "We'd like to know something about the sources that have been detected and measured, beyond a simple total number. We create a simple histogram of the Kron flux of sources in our _i_-band tile. You can find more details on merging catalogs across bands, etc on the [Science Pipelines documentation](https://pipelines.lsst.io/getting-started/multiband-analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(coadd_sources['ext_photometryKron_KronFlux_instFlux'],bins=np.linspace(0,100,50))\n",
    "plt.xlabel(\"log10(Kron Flux)\")\n",
    "plt.title(\"{filter} {tract} {patch}\".format(filter=filter,tract=tract,patch=patch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We have shown a few techniques for exploring a data repo. ~~To make this process straightforward, we are implementing all these techniques into methods of a `Taster` class, which is now a part of the `stackclub` library. The `Taster` will give you a taste of what the `Butler` delivers. we demonstrate the use of this class in the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/dc2_gen2/Graveyard/DataInventory.ipynb) notebook.~~\n",
    "\n",
    "(Note:  the `Taster` class and the [DataInventory.ipynb](https://github.com/LSSTScienceCollaborations/StackClub/blob/dc2_gen2/Graveyard/DataInventory.ipynb) notebook are now both deprecated.  The DataInventory notebook is largely a demonstration of the `Taster` class, a useful tool that was created to augment the deficiencies of the Gen-2 butler for investigating and exploring data repos. Unfortunately, it depends on classes and repo structures that have evolved substantially over the past few years. Since the `Taster` resides outside of the DM Stack, it requires constant maintenance to be kept up-to-date with the Stack.  With the impending release of the Gen-3 butler, the `Taster` and the DataInventory notebook have been deprecated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
