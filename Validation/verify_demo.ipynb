{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Quickstart LSST Verify Demo\n",
    "\n",
    "<br>Owner: **Keith Bechtol** ([@bechtol](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@bechtol))\n",
    "<br>Minor updates by:   Douglas Tucker ([@douglasleetucker](https://github.com/LSSTScienceCollaborations/StackClub/issues/new?body=@douglasleetucker))\n",
    "<br>Last Verified to Run: **v21.0.0**\n",
    "<br>Verified Stack Release: **v21.00**, **v17.0**, **d_2019_02_04**, **d_2019_01_15**\n",
    "\n",
    "This notebook demonstrates basic functionality of the LSST Verify python package: https://github.com/lsst/verify . The notebook is based on the documentation at https://sqr-019.lsst.io/ . \n",
    "\n",
    "Another example from the LSST Systems Engineering team can be found [here](https://github.com/mareuter/notebooks/blob/master/LSST/Systems_Engineering/System_Verification_SQuaSH/System_Verification_Demo.ipynb), for which the metrics are defined [here](https://github.com/mareuter/notebooks/tree/master/LSST/Systems_Engineering/System_Verification_SQuaSH).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "After working through and studying this notebook you should be able to\n",
    "   1. Create custom metrics for your science cases and associate multiple specifications with those metrics\n",
    "   2. Evaluate metrics and store the output for subsequent analysis\n",
    "   3. Create customized summary displays for the performance on those metrics relative to your specifications\n",
    "\n",
    "### Logistics\n",
    "\n",
    "This notebook is intended to be runnable on `lsst-lsp-stable.ncsa.illinois.edu` from a local git clone of https://github.com/LSSTScienceCollaborations/StackClub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What version of the Stack am I using?\n",
    "! echo $HOSTNAME\n",
    "! eups list -s lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "import lsst.verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics and specifications\n",
    "\n",
    "There are specific rules for the directory structure and naming. In particular, there are required folders for \"metrics\" and \"specs\". The metrics are defined in a set of yaml files in the metrics folder. For each yaml file of metrics, there is a corresponding directory with the same name in the specifications directory. The specifications are defined in their own yaml files. An example directory structure appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree verify_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at both the metric and specification yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat verify_demo/metrics/demo_astrometry.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat verify_demo/specs/demo_astrometry/specs.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the definition of specifications, note that the \"---\" line between specifications is required.\n",
    "\n",
    "Next, we create instances of the `MetricSet` and `SpecificationSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_PACKAGE = \"verify_demo\"\n",
    "metrics = lsst.verify.MetricSet.load_metrics_package(METRIC_PACKAGE)\n",
    "specs = lsst.verify.SpecificationSet.load_metrics_package(METRIC_PACKAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the metrics that have been defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the specifications that have been defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing and storing metrics\n",
    "\n",
    "For the purpose of illustration, let's make up some measurement values corresponding to our metrics. The following lines are placeholders for the analysis that we would want to do. In this example, we choose measurement values that are intermediate between the specifications defined above so that we can see what happens when some specifications are met and others are not. Notice that the measurements can have dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zp_rms = 15.*u.mmag\n",
    "zp_meas = lsst.verify.Measurement('demo_photometry.ZeropointRMS', zp_rms)\n",
    "\n",
    "astro_rms = 15.*u.mas\n",
    "astro_meas = lsst.verify.Measurement('demo_astrometry.AstrometricRMS', astro_rms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to include extra information along with the measurements. These are made up values only for the purpose of illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zp_meas.extras['x'] = lsst.verify.Datum(np.random.random(10) * u.mag, label=\"x\", description=\"x-values\")\n",
    "zp_meas.extras['y'] = lsst.verify.Datum(np.random.random(10) * u.mag, label=\"y\", description=\"y-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an LSST verify job and add the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = lsst.verify.Job(metrics=metrics, specs=specs)\n",
    "job.measurements.insert(zp_meas)\n",
    "job.measurements.insert(astro_meas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide metadata about the job. This could be used to capture information about the analysis configuration, software version, dataset, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta.update({'version': 'test'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are done, write the output to a file. This can be exported to metric aggregators at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.write('demo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Creating reports\n",
    "\n",
    "Create a report to visualize the outcome of our analysis. We already have the job in memory, but for the purpose of illustration, let's read in the file that we just wrote to show how one could examine the results at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('demo.json') as f:\n",
    "    job = lsst.verify.Job.deserialize(**json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.report().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that because of the measurement values we used in this example, some of the specifications are met, while others are not.\n",
    "\n",
    "It is possible to select particular tags to customize the report. The example below shows a selection on specification tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.report(spec_tags=['minimum']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to see what tags are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.metrics['demo_astrometry.AstrometricRMS'].tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of information is available for plotting if we want to dig deeper into the results. These are the extra data that we saved together with the metric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = job.measurements['demo_photometry.ZeropointRMS']\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(m.extras['x'].quantity, m.extras['y'].quantity)\n",
    "plt.xlabel('%s (%s)'%(m.extras['x'].label, m.extras['x'].unit.name))\n",
    "plt.ylabel('%s (%s)'%(m.extras['y'].label, m.extras['y'].unit.name))\n",
    "plt.title('%s; %s'%(m.metric_name.metric, job.meta[\"version\"]))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the particular values used in this example are just for demonstration purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
